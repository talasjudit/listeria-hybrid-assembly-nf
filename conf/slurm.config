/*
========================================================================================
    SLURM Configuration
========================================================================================
    Configuration for running the pipeline on HPC systems with SLURM scheduler

    Customization Required:
    - Adjust partition/queue names for your HPC system
    - Modify clusterOptions to add account/project information if required
    - Tune queueSize and submitRateLimit based on your cluster policies

    Usage:
      nextflow run main.nf -profile slurm,singularity --input samplesheet.csv
========================================================================================
*/

process {
    executor = 'slurm'

    /*
     * Queue/Partition Selection
     * Automatically selects partition based on time requirements
     *
     * TODO: CUSTOMIZE THESE PARTITION NAMES FOR YOUR HPC SYSTEM
     *
     * Common partition naming schemes:
     * - qib-short / qib-medium / qib-long
     * - short / medium / long
     * - normal / long / week
     * - compute / compute-long
     *
     * Check your system with: sinfo -o "%P %l"
     */
    queue = { task.time <= 4.h ? 'qib-short' : 'qib-medium' }

    /*
     * Additional SLURM Options
     * Add custom SLURM directives here
     *
     * Common options:
     * --account=PROJECT_NAME  : Required on many HPC systems for accounting
     * --qos=QUALITY_OF_SERVICE : Quality of service (if used by your system)
     * --partition=PARTITION    : Override automatic partition selection
     * --constraint=FEATURE     : Request nodes with specific features
     *
     * Examples:
     * clusterOptions = '--account=myproject'
     * clusterOptions = '--account=myproject --qos=normal'
     * clusterOptions = '--constraint=avx2'
     *
     * TODO: UNCOMMENT AND CUSTOMIZE IF YOUR SYSTEM REQUIRES AN ACCOUNT
     */
    clusterOptions = ''
    // clusterOptions = '--account=YOUR_PROJECT_ACCOUNT'

    /*
     * SLURM Job Options
     */
    // Use job arrays for better scheduling (if supported by your cluster)
    // arraySize = 100  // Uncomment to enable job arrays

    // Additional options for job submission
    // beforeScript = 'module load singularity'  // Uncomment if Singularity needs to be loaded
}

/*
========================================================================================
    EXECUTOR SETTINGS
========================================================================================
*/

executor {
    /*
     * Queue Size - Maximum number of jobs to submit at once
     * Adjust based on your cluster policies and quota
     *
     * Conservative: 20-50 jobs
     * Moderate: 50-100 jobs
     * Aggressive: 100+ jobs
     */
    queueSize = 50

    /*
     * Submit Rate Limit - Control how fast jobs are submitted
     * Prevents overwhelming the SLURM scheduler
     *
     * Format: 'N jobs / time unit'
     * Examples:
     * - '10 sec' : Submit up to 10 jobs per second
     * - '5 min'  : Submit up to 5 jobs per minute
     * - '100/2min' : Submit 100 jobs over 2 minutes
     */
    submitRateLimit = '10 sec'

    /*
     * Poll Interval - How often to check job status
     * Default is usually fine, but can be increased on busy systems
     */
    // pollInterval = '5 sec'

    /*
     * Queue Stat Interval - How often to retrieve full queue status
     * Can be increased on large busy systems to reduce scheduler load
     */
    // queueStatInterval = '1 min'
}

/*
========================================================================================
    SINGULARITY SETTINGS
========================================================================================
*/

singularity {
    enabled     = true
    autoMounts  = true
    cacheDir    = params.singularity_cachedir

    /*
     * Additional Singularity Options
     * Uncomment and customize as needed for your system
     */

    // Bind additional paths (if needed for accessing data or databases)
    // runOptions = '--bind /path/to/data:/data --bind /path/to/scratch:/scratch'

    // Use $TMPDIR for temporary files (recommended on many HPC systems)
    // envWhitelist = 'TMPDIR'

    // Specify library and cache directories (useful for shared installations)
    // libraryDir = '/shared/singularity/library'
}

/*
========================================================================================
    NOTES FOR CUSTOMIZATION
========================================================================================

    1. Finding Your Partition Names:
       Run: sinfo -o "%P %l"
       This shows available partitions and their time limits

    2. Checking If Account Is Required:
       Run: sacctmgr show user $USER format=account
       If you see account names, you likely need to specify one

    3. Testing Your Configuration:
       Start with a small test:
       nextflow run main.nf -profile slurm,singularity,test

    4. Common Issues:
       - "Invalid account" : Add your project account to clusterOptions
       - "Invalid partition" : Update queue names to match your system
       - "QOS not allowed" : Remove or update QOS settings
       - "Time limit exceeded" : Jobs are taking longer than partition allows

    5. Monitoring:
       - Check queue: squeue -u $USER
       - Check job details: scontrol show job JOBID
       - Check accounting: sacct -j JOBID --format=JobID,JobName,MaxRSS,Elapsed

========================================================================================
*/
