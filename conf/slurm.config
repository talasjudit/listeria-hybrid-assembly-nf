/*
========================================================================================
    SLURM Configuration (Generic)
========================================================================================
    Generic configuration for running the pipeline on any HPC with SLURM scheduler

    This profile provides:
    - SLURM executor settings
    - Singularity container support
    - Offline mode for compute nodes
    - Sensible job submission defaults

    For site-specific partition names, use an institutional profile like 'qib'
    or create your own custom config file.

    Usage:
      nextflow run main.nf -profile slurm --input samplesheet.csv
      nextflow run main.nf -profile slurm -c my_hpc.config --input samplesheet.csv
========================================================================================
*/

process {
    executor = 'slurm'

    /*
     * Queue/Partition Selection
     * 
     * By default, uses SLURM's default partition.
     * Override with:
     *   1. Custom config file: -c my_hpc.config
     *   2. Command line: --queue 'partition_name'
     *   3. Institutional profile: -profile qib
     *
     * Check available partitions: sinfo -o "%P %l"
     */
    // queue = { task.time <= 2.h ? 'short' : 'long' }

    /*
     * Additional SLURM Options
     *
     * Common options:
     * --account=PROJECT  : Required on many HPC systems
     * --qos=LEVEL        : Quality of service
     * --constraint=FEAT  : Request specific node features
     */
    clusterOptions = ''
    // clusterOptions = '--account=YOUR_PROJECT_ACCOUNT'
}

/*
========================================================================================
    EXECUTOR SETTINGS
========================================================================================
*/

executor {
    /*
     * Queue Size - Maximum concurrent jobs submitted
     * Adjust based on your cluster policies and quota
     */
    queueSize = 50

    /*
     * Submit Rate Limit - Prevent overwhelming the scheduler
     */
    submitRateLimit = '10 sec'
}

/*
========================================================================================
    OFFLINE MODE FOR COMPUTE NODES
========================================================================================
    HPC compute nodes typically don't have internet access.
    Run the installation workflow on a login node first.
*/

env {
    NXF_OFFLINE = 'true'
    NXF_DISABLE_REMOTE_LOGS = 'true'
}

/*
========================================================================================
    SINGULARITY SETTINGS
========================================================================================
*/

singularity {
    enabled     = true
    autoMounts  = true
    cacheDir    = params.singularity_cachedir

    /*
     * Additional options (uncomment as needed):
     */
    // runOptions = '--bind /path/to/data:/data'
    // envWhitelist = 'TMPDIR'
}

/*
========================================================================================
    CUSTOMIZATION GUIDE
========================================================================================

    1. Finding Your Partition Names:
       sinfo -o "%P %l"

    2. Checking If Account Is Required:
       sacctmgr show user $USER format=account

    3. Creating a Custom Config:
       Create my_hpc.config with:
       
       process {
           queue = { task.time <= 2.h ? 'short' : 'long' }
           clusterOptions = '--account=myproject'
       }

       Then run:
       nextflow run main.nf -profile slurm -c my_hpc.config --input samplesheet.csv

    4. Common Issues:
       - "Invalid partition" : Set queue to match your system
       - "Invalid account"   : Add account to clusterOptions
       - "Time limit exceeded": Jobs exceeding partition limits

========================================================================================
*/
